{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.8 64-bit"
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "colab": {
      "name": "gotcha.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##2021-07-26 log\n",
        "* y 값 설정해주는걸 좀 고심해서 해봐야 할 것 같다. 어떻게 할지? 아무래도 데이터 불러오는걸 따로 함수로 빼야할 것 같음.\n",
        "* CNN feature extractor 수정이 필요함.\n",
        "* 논문에서는 LSTM으로 했지만 굳이 LSTM으로 할 필요가 있나.. 하는 생각이 든다. RNN 으로 해도 될듯."
      ],
      "metadata": {
        "id": "nZeq4PvSu1l8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DdIpQ9eP-Wy",
        "outputId": "02caaaa5-d98b-4eb1-fdf4-915bdb0c9a2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!mkdir \"/content/train_data/\"\n",
        "!unzip \"/content/gdrive/MyDrive/GOTCHA/training_07_30.zip\" -d /content/train_data"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEsrJbplP9T_",
        "outputId": "fc2670e4-cd40-4d53-85cf-c89a3ab840af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pre-data\n",
        "* updated : 2021-07-20\n"
      ],
      "metadata": {
        "id": "4klNuDH6T_2L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "from keras import backend as K\n",
        "\n",
        "from keras.layers import Input, Dense, Flatten, Activation, Dropout, Bidirectional, Permute, multiply\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "import glob"
      ],
      "outputs": [],
      "metadata": {
        "id": "DJIkM8aGT_2N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "K.set_image_data_format('channels_first')"
      ],
      "outputs": [],
      "metadata": {
        "id": "rRsEgDrLT_2S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "NUM_EPOCHS = 60\n",
        "VERBOSE = 1\n",
        "HIDDEN_UNITS = 3"
      ],
      "outputs": [],
      "metadata": {
        "id": "a_eXdiD4UEVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "def attention_block(inputs, time_steps):\n",
        "    a = Permute((2, 1))(inputs)\n",
        "    a = Dense(time_steps, activation='softmax')(a)\n",
        "    a_probs = Permute((2, 1), name='attention_vec')(a) ## transpose 후 softmax를 빠져나와 다시 한번 더 transpose를 해줌으로서, 중요한 부분에만 가중치가 가도록 하는 역할.\n",
        "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul') ## input과 attention probability를 곱함.\n",
        "    return output_attention_mul ## attention module 을 통과한 것을 내보냄."
      ],
      "outputs": [],
      "metadata": {
        "id": "GRIfsChZUFh5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "import os \n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "#/content/train_data/training/del/father/chr10_s.11780473_vp.11780468.png\n",
        "def generate_batch(path, batch_size):\n",
        "    total_x = glob.glob(path + \"/**/father/*.png\", recursive = True)\n",
        "    print(len(total_x))\n",
        "    num_batches = len(total_x) // batch_size\n",
        "    while True:\n",
        "      for batchIdx in range(0, num_batches):\n",
        "        start = batchIdx * batch_size\n",
        "        end = (batchIdx + 1 )* batch_size\n",
        "        print(start, end)\n",
        "        x_data = []\n",
        "        y_data = []\n",
        "        \n",
        "        for k in range(start, end):\n",
        "          info = total_x[k].split(\"/\")\n",
        "          x = np.array(Image.open(total_x[k]))\n",
        "          temp = np.array([np.array(Image.open(\"/\".join(info[0:5] + [\"proband\"] + [info[-1]]))), \n",
        "                          x ,\n",
        "                          np.array(Image.open(\"/\".join(info[0:5] + [\"mother\"] + [info[-1]])))])\n",
        "          \n",
        "          pos = np.zeros((64,1))\n",
        "          pos[int((info[-1].split(\"_\")[1]).split(\".\")[1]) - int((info[-1].split(\"_\")[2]).split(\".\")[1])] = 1\n",
        "          \n",
        "          y_data.append(pos)\n",
        "          x_data.append(temp)\n",
        "        \n",
        "        yield np.array(x_data), y_data\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "iShsbOEwUGmc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "generate_batch(\"/content/train_data/training\", 32)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object generate_batch at 0x7f8bd563fbd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lq3LO1bRXKZ",
        "outputId": "9f8dfeb6-b14b-4aa1-dfd8-4f2cc43e2577"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BidirectionalLSTMClassifier\n",
        "*updated : 2021-07-26\n"
      ],
      "metadata": {
        "id": "cRcszkCiUYiX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class BidirectionalLSTMClassifier(object):\n",
        "  def __init__(self, cnn_model_name, model_file = None):\n",
        "    self.num_input_tokens = None\n",
        "    self.nb_classes = None\n",
        "    if model_file is None: self.model = None\n",
        "    else:self.model = load_model(model_file)\n",
        "\n",
        "    self.cnn_model_name = cnn_model_name.lower()\n",
        "\n",
        "    def cnn_attention_lstm(self):\n",
        "        inputs = Input(shape=(self.expected_frames, self.num_input_tokens,))\n",
        "        attention_inputs = attention_block(inputs, self.expected_frames)\n",
        "        lstm_out = Bidirectional(SimpleRNN(HIDDEN_UNITS, return_sequences=False))(attention_inputs) ## LSTM --> simple RNN\n",
        "        x = Dense(512, activation='relu')(lstm_out)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(self.nb_classes, activation='softmax')(x)\n",
        "        model = Model(input=[inputs], output=x)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "        print(model.summary())\n",
        "        return model\n",
        "\n",
        "    def fit(self, input_path, output_path, data_set_name = '' , test_size = 0.3, attention = 'cnn_attention_lstm', do_feature_extraction = False):\n",
        "      batch_size = 128\n",
        "\n",
        "      cnn_model = InceptionV3(include_top=False, weights='imagenet')\n",
        "      self.cnn_model_name = 'InceptionV3'\n",
        "      feature_dir_name = data_set_name + '-' +  self.cnn_model_name + '-HiDimFeatures'\n",
        "\n",
        "      labels = dict()\n",
        "\n",
        "      feature_extractor = CNN_feature_extractor(self.cnn_model_name)\n",
        "      x_samples, y_samples = feature_extractor.extract_cnn_features(input_path, \n",
        "                                                                    output_dir_path = feature_dir_name,\n",
        "                                                                    model = cnn_model,\n",
        "                                                                    data_set_name = data_set_name)\n",
        "      \n",
        "      if do_feature_extraction : return\n",
        "\n",
        "      x_sample_0 = np.load(x_samples[0])\n",
        "      print(x_sample_0.shape)\n",
        "\n",
        "      self.num_input_tokens = x_sample_0.shape[1]\n",
        "\n",
        "      frames_list = []\n",
        "\n",
        "      for x in x_samples:\n",
        "        frames = np.load(x).shape[0]\n",
        "        frames_list.append(frames)\n",
        "      \n",
        "      self.expected_frames = int(np.mean(frames_list))\n",
        "      print('expected frames : ', self.expected_frames)\n",
        "\n",
        "      ########################################################\n",
        "      for y in y_samples:\n",
        "        if y not in labels:\n",
        "          labels[y] = len(labels)\n",
        "      \n",
        "      for i in range(len(y_samples)):\n",
        "        y_samples[i] = labels[y_samples[i]]\n",
        "\n",
        "      self.nb_classes = len(labels)\n",
        "\n",
        "      y_samples = np_utils.to_categorical(y_samples, self.nb_classes)\n",
        "\n",
        "      ######################################################## \n",
        "\n",
        "      config = dict()\n",
        "      config['labels'] = labels\n",
        "      config['nb_classes'] = self.nb_classes\n",
        "      config['num_input_tokens'] = self.num_input_tokens\n",
        "      config['expected_frames'] = self.expected_frames\n",
        "\n",
        "      np.save(os.path.join(output_path, 'config'), config)\n",
        "      \n",
        "      t1 = time.time()\n",
        "      model = self.cnn_attention_lstm()\n",
        "      csv_logger = CSVLogger(os.path.join(output_path, '{}_cnn_attention_lstm.log'.format(self.cnn_model_name)), append=True,\n",
        "                                   separator=';')\n",
        "      Xtrain, Xtest, Ytrain, Ytest = train_test_split(x_samples, y_samples, test_size=test_size,\n",
        "                                                        random_state=None)\n",
        "      \n",
        "      train_gen = generate_batch(Xtrain, Ytrain, batch_size, self.expected_frames)\n",
        "      test_gen = generate_batch(Xtest, Ytest, batch_size, self.expected_frames)\n",
        "\n",
        "      train_num_batches = len(Xtrain) // batch_size\n",
        "      test_num_batches = len(Xtest) // batch_size\n",
        "\n",
        "      history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
        "                                    epochs=NUM_EPOCHS,\n",
        "                                    verbose=1, validation_data=test_gen, validation_steps=test_num_batches,\n",
        "                                    callbacks=[csv_logger])\n",
        "      \n",
        "      model_file_path = os.path.join(output_path, attention.replace('cnn', self.cnn_model_name) + '.h5')\n",
        "      model.save(model_file_path)\n",
        "      accu = history.history['val_acc'][-1]\n",
        "      print('cnn-{}, attention-{}: accuracy-{}, time={}'.format(self.cnn_model_name, attention, accu, time.time()-t1))\n",
        "\n",
        "      return accu\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "IWl-4xJ-UHI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN feature selector\n",
        "* updated : 2021-07-26"
      ],
      "metadata": {
        "id": "e_FEFe-AUOpi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class CNN_Feature_extractor():\n",
        "  def __init__ (self, cnn_model_name):\n",
        "    self.cnn_model_name = cnn_model_name\n",
        "\n",
        "  def extract_cnn_features_live():\n",
        "\n",
        "  def cnn_features():\n",
        "\n",
        "  def extract_cnn_features(self, input_data_dir_path, output_dir_path, model, data_set_name):\n",
        "    y_samples = []\n",
        "    x_samples = []\n",
        "\n",
        "    for f in os.listdir(input_data_dir_path):\n",
        "      file_path = input_data_dir_path + os.path.sep + f\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "U5dcUoGRUIsD"
      }
    }
  ]
}